{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiu14c/NLI-RoBERTa/blob/main/nlu_cw_transformer_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README\n",
        "\n",
        "## The model\n",
        "This is a model for Natural Language Inference. It uses a pre-trained RoBERTa model. RoBERTa has the same architecture as Bert, but uses a BPE tokenizer instead. Some hyperparameters and the pre-training objectives are also changed. RoBERTa is not pre-trained using the Next Sentence Predition task.\n",
        "\n",
        "A classification head consisting of a dense, a drop-out and another dense layer is added on top of this. The pre-trained RoBERTa model produces encodings of the input. The classification head takes them and procuses the probabilities of each class (class 0 -> no implication, class 1 -> hypothesis implies premise).\n",
        "\n",
        "This entire model has-been fine-tuned on our data set. Both the parameters of the RoBERTa base model and those of the untrained Classification Head have been changed.\n",
        "\n",
        "## Credits\n",
        "\n",
        "The architecture was selected based on a similar model's perfromance on the [RTE benchmark](https://paperswithcode.com/sota/natural-language-inference-on-rte). The code is inspired from [this article](https://pchanda.github.io/Roberta-FineTuning-for-Classification/), where a model with the same architecture was fine-tuned for classifying molecules.\n",
        "\n",
        "## Fine-tuned model location\n",
        "\n",
        "Post fine-tuning, the model has been stored on the Cloud at [this location](https://drive.google.com/file/d/1-IJSt2HGH9Dqbu6NBuHr61ndV1r4g-3H/view?usp=sharing).  It can be downloaded and used directly in the notebook, but uploading it to Colab takes more than 15 minutes. Hence a link to Google Drive was used in the code for loading the model during testing. However, if one wants to test this notebook, this link needs to be replaced by the location of the downloaded model on their machine."
      ],
      "metadata": {
        "id": "qi7tQIjJUl9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo a fine-tuned RoBERTa model\n",
        "\n",
        "##Pre-requisites\n",
        "\n",
        "\n",
        "\n",
        "*   The test data should be a csv file with 2 columns with the labels 'premise' and 'hypothesis'.\n",
        "*   The test data csv should be called 'test.csv' and should be loaded into Colab. Alternatively, one can modify the *test_location* variable, which holds both the location and name of the csv file. Links to Google Drive can be used.\n",
        "*   The fine-tuned model is currently loaded from Google Drive. If one wants to test this notebook, please download the model from [here](https://drive.google.com/file/d/1-IJSt2HGH9Dqbu6NBuHr61ndV1r4g-3H/view?usp=sharing) and change the content of the variable *PATH* to the location of the downloaded model.\n",
        "* The usage of a GPU runtime is encouraged for large test files, but it is not needed for less than 1000 test samples.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TYmgHIemPyXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries."
      ],
      "metadata": {
        "id": "3Zjvh5EhPnWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PyFJK3YKSCEx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader,\n",
        "    RandomSampler,\n",
        "    SequentialSampler\n",
        ")\n",
        "\n",
        "import math\n",
        "from transformers import  (\n",
        "    BertPreTrainedModel,\n",
        "    RobertaConfig,\n",
        "    RobertaTokenizerFast\n",
        ")\n",
        "\n",
        "from transformers.optimization import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "from scipy.special import softmax\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    matthews_corrcoef,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    average_precision_score,\n",
        ")\n",
        "\n",
        "from transformers.models.roberta.modeling_roberta import (\n",
        "    RobertaClassificationHead,\n",
        "    RobertaConfig,\n",
        "    RobertaModel,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some hyperparameters."
      ],
      "metadata": {
        "id": "_gVShKXJP944"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1vtMf_qdSPlP"
      },
      "outputs": [],
      "source": [
        "tokenizer_name = 'FacebookAI/roberta-base'\n",
        "num_labels = 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "max_seq_length = 128\n",
        "train_batch_size = 64\n",
        "test_batch_size = 64\n",
        "warmup_ratio = 0.06\n",
        "weight_decay=10**(-5)\n",
        "gradient_accumulation_steps = 1\n",
        "num_train_epochs = 10\n",
        "learning_rate = 1e-05\n",
        "adam_epsilon = 1e-08"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load tokenizer"
      ],
      "metadata": {
        "id": "ksazSpHfsGFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_class = RobertaTokenizerFast\n",
        "tokenizer = tokenizer_class.from_pretrained(tokenizer_name, do_lower_case=False)\n",
        "print('Tokenizer=',tokenizer,'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4U4C9KWsHkR",
        "outputId": "304b7fc8-9e2a-4f78-ff5c-b32bec6ec3b4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer= RobertaTokenizerFast(name_or_path='FacebookAI/roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redefine the model class"
      ],
      "metadata": {
        "id": "wZ-17zzmQFRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qmD9GeQmSSHb"
      },
      "outputs": [],
      "source": [
        "class RobertaClassifier(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(RobertaClassifier, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids,attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]\n",
        "\n",
        "        outputs = outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class that tokenizes the input data and converts it to a PyTorch tensors"
      ],
      "metadata": {
        "id": "2v2S03DJSc98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "unAKaz-gSa0D"
      },
      "outputs": [],
      "source": [
        "class NliDataset(Dataset):\n",
        "    def __init__(self, text, tokenizer):\n",
        "        self.examples = tokenizer(text=text,text_pair=None,truncation=True,padding=\"max_length\",\n",
        "                                  max_length=max_seq_length,return_tensors=\"pt\")\n",
        "        print(self.examples['input_ids'].shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {key: self.examples[key][index] for key in self.examples}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and process test data\n",
        "\n",
        "Column 1: '< s > hypothesis < s > premise <\\s>'\n",
        "\n",
        "Column 2: label (0/1)"
      ],
      "metadata": {
        "id": "GB54MSRL_VlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group training data into batches."
      ],
      "metadata": {
        "id": "bIPRX6eTS8ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "def get_data(location):\n",
        "  df = pd.read_csv(location)\n",
        "\n",
        "  #show some data before processing\n",
        "  print(\"\\nBefore:\\n\")\n",
        "  print(\"Columns: \", df.columns)\n",
        "  print(\"\\nFirst entry:\\n \", df.iloc[1])\n",
        "\n",
        "  #join the premise and hypothesis columns using separation tokens <s> and </s>\n",
        "  df['text'] = \" <s> \" + df['premise'] + \" </s> \" + df['hypothesis'] + \" </s> \"\n",
        "  df.drop(columns=['premise','hypothesis'], inplace=True)\n",
        "  df = df[['text']]\n",
        "\n",
        "  #show some data before processing\n",
        "  print(\"\\nAfter:\\n\")\n",
        "  print(\"Columns: \", df.columns)\n",
        "  print(\"\\nFirst entry:\\n \", df.iloc[1])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Xs7ayFLEMCky"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read and process the data\n",
        "test_location = 'test.csv'\n",
        "print('\\n\\nTest data:')\n",
        "test_df = get_data(test_location)\n",
        "\n",
        "#the batch size that gets processed at once during testing\n",
        "test_batch_size = 64\n",
        "\n",
        "test_examples = (test_df.iloc[:, 0].astype(str).tolist())\n",
        "test_dataset = NliDataset(test_examples,tokenizer)\n",
        "\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)"
      ],
      "metadata": {
        "id": "l2d3xeb6MFAE",
        "outputId": "76261232-8366-4db2-a401-ae80f7b03e4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Test data:\n",
            "\n",
            "Before:\n",
            "\n",
            "Columns:  Index(['premise', 'hypothesis'], dtype='object')\n",
            "\n",
            "First entry:\n",
            "  premise       He really shook up my whole mindset, Broker says. \n",
            "hypothesis               His mindset never changed, Broker said.\n",
            "Name: 1, dtype: object\n",
            "\n",
            "After:\n",
            "\n",
            "Columns:  Index(['text'], dtype='object')\n",
            "\n",
            "First entry:\n",
            "  text     <s> He really shook up my whole mindset, Broker says.  </s> His mindset never changed, Broker said. </s> \n",
            "Name: 1, dtype: object\n",
            "torch.Size([20, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the fine-tuned model"
      ],
      "metadata": {
        "id": "y4L4rAe8V2zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/drive/MyDrive/NLU/roberta-model4.pt\"\n",
        "model = torch.load(PATH, map_location=device)\n",
        "# model = torch.load(PATH)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7P8iLWaWlUx",
        "outputId": "32f3745e-40dc-4a29-b0a5-5a86f41cf900"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaClassifier(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make the test predictions."
      ],
      "metadata": {
        "id": "GwXWL_gmXY03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "n_batches = len(test_dataloader)\n",
        "preds = np.empty((len(test_dataset), num_labels))\n",
        "out_label_ids = np.empty((len(test_dataset)))\n",
        "model.eval()\n",
        "\n",
        "print(len(test_dataloader))\n",
        "for i,test_batch in enumerate(test_dataloader):\n",
        "    with torch.no_grad():\n",
        "        if i%10 ==0:\n",
        "          print(i)\n",
        "\n",
        "        input_ids = test_batch['input_ids'].to(device)\n",
        "        attention_mask = test_batch['attention_mask'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[:1]\n",
        "\n",
        "    start_index = test_batch_size * i\n",
        "    end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
        "    preds[start_index:end_index] = np.array(tuple(t.cpu() for t in logits))\n",
        "\n",
        "model_outputs = preds\n",
        "\n",
        "preds = np.argmax(preds, axis=1)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Utqc2HDjlbVD",
        "outputId": "fb482c4d-4ed4-4d82-ec33-9edc815aad56"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n",
            "[1 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write predictions to a csv file."
      ],
      "metadata": {
        "id": "xizsit0PYL0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_df = pd.DataFrame({'prediction':preds})\n",
        "preds_df.to_csv('test-predictions.csv', index=False)\n",
        "# preds_df.to_csv('drive/MyDrive/NLU/test-predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "eIyPK1TiYO9i"
      },
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}