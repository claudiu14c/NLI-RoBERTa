{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiu14c/NLI-RoBERTa/blob/main/nlu_cw_transformer_training_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README\n",
        "\n",
        "## The model\n",
        "This is a model for Natural Language Inference. It uses a pre-trained RoBERTa model. Roberta has the same architecture as Bert, but uses a BPE tokenizer instead. Some hyperparameters and the pre-training objectives are also changed. RoBERTa is not pre-trained using the Next Sentence Predition task.\n",
        "\n",
        "A classification head consisting of a dense, a drop-out and another dense layer is added on top of this. The pre-trained RoBERTa model produces encodings of the input. The classification head takes them and procuses the probabilities of each class (class 0 -> no implication, class 1 -> hypothesis implies premise).\n",
        "\n",
        "This entire model is fine-tuned on our data set. Both the parameters od the RoBERTa model and those of the Classification Head are changed.\n",
        "\n",
        "## Credits\n",
        "\n",
        "This architecture was selected based on a similar model's perfromance on the [RTE benchmark](https://paperswithcode.com/sota/natural-language-inference-on-rte). The code is inspired from [this article](https://pchanda.github.io/Roberta-FineTuning-for-Classification/), where a model with the same architecture was fine-tuned for classifying molecules.\n",
        "\n",
        "## Fine-tuned model location\n",
        "\n",
        "Post fine-tuning, the model has been stored on the Cloud at [this location](https://drive.google.com/file/d/1-IJSt2HGH9Dqbu6NBuHr61ndV1r4g-3H/view?usp=sharing).  It can be downloaded and used directly in the notebook, but uploading it to Colab takes more than 15 minutes. Hence a link to Google Drive was used in the code for loading the model during development. However, if one wants to test this notebook, this link needs to be replaced by the location of the downloaded model during the evaluation step.\n",
        "\n",
        "## Evaluation\n",
        "The model is evaluated both during and after training. Training is stopped if the model performance on the evaluation set stops improving. The evaluation metrics computed are:\n",
        "\n",
        "*  Accuracy\n",
        "*  Macro and Weighted Precision, Recall and F1 scores\n",
        "*  Mathew's Correlation Coefficient\n",
        "*  TP, TN, FP, FN"
      ],
      "metadata": {
        "id": "Y1EsRGvN9DyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning a RoBERTa model with a classification head on top\n",
        "##Pre-requisites\n",
        "\n",
        "\n",
        "\n",
        "*   The training and evaluation data should be in csv files with 3 columns  labeled 'premise', 'hypothesis', and 'label'.\n",
        "*   The traning data csv should be called 'train.csv', and the evaluation data should be called 'dev.csv'. Both should be loaded into Colab. Alternatively, one can modify the *tr_location* and *dev_location* variables, which hold both the location and name of the csv files. Links to Google Drive can be used.\n",
        "*   The final evaluation can be done either on the same fine-tuned model or on another model loaded from the file sytem or Google Drive. Since this notebook had already been run, the fine-tuned model is currently loaded from Google Drive. If one wants to test the evaluation step, please download the fine-tuned model from [here](https://drive.google.com/file/d/1-IJSt2HGH9Dqbu6NBuHr61ndV1r4g-3H/view?usp=sharing) and change the content of the variable *MODEL_PATH* to the location of the downloaded model.\n",
        "* A GPU environment must be used for training.\n"
      ],
      "metadata": {
        "id": "TYmgHIemPyXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries and configuration files for RoBERTa."
      ],
      "metadata": {
        "id": "3Zjvh5EhPnWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "PyFJK3YKSCEx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader,\n",
        "    RandomSampler,\n",
        "    SequentialSampler\n",
        ")\n",
        "\n",
        "import math\n",
        "from transformers import  (\n",
        "    BertPreTrainedModel,\n",
        "    RobertaConfig,\n",
        "    RobertaTokenizerFast\n",
        ")\n",
        "\n",
        "from transformers.optimization import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "from scipy.special import softmax\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    matthews_corrcoef,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    average_precision_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "\n",
        "from transformers.models.roberta.modeling_roberta import (\n",
        "    RobertaClassificationHead,\n",
        "    RobertaConfig,\n",
        "    RobertaModel,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to read training (also dev) data into a csv file.\n",
        "Important: the data is pre-processed and separation tokens (< s >, < /s >) are added. These tokens are the equivalemnts of < CLS > and < SEP > in a classic Bert model.\n",
        "The data is saved into a dataframe of the following form:\n",
        "\n",
        "\n",
        "*   Column 1: 'text': '< s > premise < s > hypothesis < /s >'\n",
        "*   Column 2: 'label': label (0/1)\n"
      ],
      "metadata": {
        "id": "nSIZFJrpP3jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "def get_data(location):\n",
        "  df = pd.read_csv(location)\n",
        "\n",
        "  #show some data before processing\n",
        "  print(\"\\nBefore:\\n\")\n",
        "  print(\"Columns: \", df.columns)\n",
        "  print(\"\\nFirst entry:\\n \", df.iloc[1])\n",
        "\n",
        "  #join the premise and hypothesis columns using separation tokens <s> and </s>\n",
        "  df['text'] = \" <s> \" + df['premise'] + \" </s> \" + df['hypothesis'] + \" </s> \"\n",
        "  df.drop(columns=['premise','hypothesis'], inplace=True)\n",
        "  df = df[['text', 'label']]\n",
        "\n",
        "  #show some data before processing\n",
        "  print(\"\\nAfter:\\n\")\n",
        "  print(\"Columns: \", df.columns)\n",
        "  print(\"\\nFirst entry:\\n \", df.iloc[1])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Kng4SI5pB2oq"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read and process train and dev data.\n",
        "\n",
        "The 'location' variable is the link from which the data is read. Drive links can be used.\n",
        "\n",
        "In this examples, it is assumed that the train and evalutation data are called \"train.csv\" and \"test.csv\" and are loaded into Colab."
      ],
      "metadata": {
        "id": "NNla6kBdCwaf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxZEfF556Uif",
        "outputId": "38a2a63e-b977-43a0-b61d-aec29c728e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data\n",
            "\n",
            "Before:\n",
            "\n",
            "Columns:  Index(['premise', 'hypothesis', 'label'], dtype='object')\n",
            "\n",
            "First entry:\n",
            "  premise       Buchanan's  The Democrats and Republicans have become too similar and bland.\n",
            "hypothesis                                              THe parties will never be similar.\n",
            "label                                                                                    0\n",
            "Name: 1, dtype: object\n",
            "\n",
            "After:\n",
            "\n",
            "Columns:  Index(['text', 'label'], dtype='object')\n",
            "\n",
            "First entry:\n",
            "  text      <s> Buchanan's  The Democrats and Republicans have become too similar and bland. </s> THe parties will never be similar. </s> \n",
            "label                                                                                                                                  0\n",
            "Name: 1, dtype: object\n",
            "\n",
            "\n",
            "\n",
            "Evaluation data:\n",
            "\n",
            "Before:\n",
            "\n",
            "Columns:  Index(['premise', 'hypothesis', 'label'], dtype='object')\n",
            "\n",
            "First entry:\n",
            "  premise       He really shook up my whole mindset, Broker says. \n",
            "hypothesis               His mindset never changed, Broker said.\n",
            "label                                                          0\n",
            "Name: 1, dtype: object\n",
            "\n",
            "After:\n",
            "\n",
            "Columns:  Index(['text', 'label'], dtype='object')\n",
            "\n",
            "First entry:\n",
            "  text      <s> He really shook up my whole mindset, Broker says.  </s> His mindset never changed, Broker said. </s> \n",
            "label                                                                                                             0\n",
            "Name: 1, dtype: object\n"
          ]
        }
      ],
      "source": [
        "tr_location = 'train.csv'\n",
        "dev_location = 'dev.csv'\n",
        "\n",
        "print('Training data')\n",
        "train_df = get_data(tr_location)\n",
        "\n",
        "print('\\n\\n\\nEvaluation data:')\n",
        "dev_df = get_data(dev_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the hyperparameters."
      ],
      "metadata": {
        "id": "_gVShKXJP944"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "1vtMf_qdSPlP"
      },
      "outputs": [],
      "source": [
        "model_name = 'FacebookAI/roberta-base'\n",
        "num_labels = 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer_name = model_name\n",
        "\n",
        "max_seq_length = 128\n",
        "train_batch_size = 64\n",
        "dev_batch_size = 64\n",
        "warmup_ratio = 0.06\n",
        "weight_decay=10**(-5)\n",
        "gradient_accumulation_steps = 1\n",
        "num_train_epochs = 10\n",
        "learning_rate = 1e-05\n",
        "adam_epsilon = 1e-08"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a classification head containing 1 linear layer, one drop-out and another linear layer on top of the Roberta encodings."
      ],
      "metadata": {
        "id": "wZ-17zzmQFRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "qmD9GeQmSSHb"
      },
      "outputs": [],
      "source": [
        "class RobertaClassifier(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(RobertaClassifier, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels):\n",
        "        outputs = self.roberta(input_ids,attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]\n",
        "\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a pre-trained Roberta model with the default configuration and its tokenizer."
      ],
      "metadata": {
        "id": "g18ubBB9QO0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu6LY2BSSU_7",
        "outputId": "3e7d1d47-34de-4afc-a2df-b90a93f975d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaClassifier were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model=\n",
            " RobertaClassifier(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (classifier): RobertaClassificationHead(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "  )\n",
            ") \n",
            "\n",
            "Tokenizer= RobertaTokenizerFast(name_or_path='FacebookAI/roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
            "} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "config_class = RobertaConfig\n",
        "model_class = RobertaClassifier\n",
        "tokenizer_class = RobertaTokenizerFast\n",
        "\n",
        "config = config_class.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "model = model_class.from_pretrained(model_name, config=config)\n",
        "print('Model=\\n',model,'\\n')\n",
        "\n",
        "tokenizer = tokenizer_class.from_pretrained(tokenizer_name, do_lower_case=False)\n",
        "print('Tokenizer=',tokenizer,'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class that tokenizes the input data and converts it to a PyTorch tensors"
      ],
      "metadata": {
        "id": "2v2S03DJSc98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "unAKaz-gSa0D"
      },
      "outputs": [],
      "source": [
        "class NliDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, tokenizer):\n",
        "        text, labels = data\n",
        "        self.examples = tokenizer(text=text,text_pair=None,truncation=True,padding=\"max_length\",\n",
        "                                  max_length=max_seq_length,return_tensors=\"pt\")\n",
        "        print(self.examples['input_ids'].shape)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {key: self.examples[key][index] for key in self.examples}, self.labels[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply it to our data."
      ],
      "metadata": {
        "id": "PvRfLpzfEjqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = (train_df.iloc[:, 0].astype(str).tolist(), train_df.iloc[:, 1].tolist())\n",
        "train_dataset = NliDataset(train_examples,tokenizer)\n",
        "\n",
        "dev_examples = (dev_df.iloc[:, 0].astype(str).tolist(), dev_df.iloc[:, 1].tolist())\n",
        "dev_dataset = NliDataset(dev_examples,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3mo6pnBEhMp",
        "outputId": "8d0cfc87-ef3b-405f-ee55-e75aa6981d95"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([13, 128])\n",
            "torch.Size([13, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group training and dev data into batches."
      ],
      "metadata": {
        "id": "bIPRX6eTS8ng"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "NC_zB_WMSd4-"
      },
      "outputs": [],
      "source": [
        "def get_inputs_dict(batch):\n",
        "    inputs = {key: value.squeeze(1).to(device) for key, value in batch[0].items()}\n",
        "    inputs[\"labels\"] = batch[1].to(device)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=train_batch_size)\n",
        "\n",
        "dev_sampler = SequentialSampler(dev_dataset)\n",
        "dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=dev_batch_size)"
      ],
      "metadata": {
        "id": "RT9O0kDILNDh"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom parameters for the Adam optimiser."
      ],
      "metadata": {
        "id": "unp4I3TcS7r4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "FuydGA4yTNID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f9997f-3e8f-4161-eaa7-082095326751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
        "optimizer_grouped_parameters = []\n",
        "custom_parameter_names = set()\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters.extend(\n",
        "    [\n",
        "        {\n",
        "            \"params\": [\n",
        "                p\n",
        "                for n, p in model.named_parameters()\n",
        "                if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\n",
        "                p\n",
        "                for n, p in model.named_parameters()\n",
        "                if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "\n",
        "warmup_steps = math.ceil(t_total * warmup_ratio)\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to compute the desired evaluation metrics:\n",
        "\n",
        "\n",
        "*  Accuracy\n",
        "*  Macro and Weighted Precision, Recall and F1 scores\n",
        "*  Mathew's Correlation Coefficient\n",
        "*  TP, TN, FP, FN\n",
        "\n"
      ],
      "metadata": {
        "id": "6RrhvzoxTeMe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "G6HRg_tdTP0U"
      },
      "outputs": [],
      "source": [
        "def print_confusion_matrix(result):\n",
        "    print('confusion matrix:')\n",
        "    print('            predicted    ')\n",
        "    print('          0     |     1')\n",
        "    print('    ----------------------')\n",
        "    print('   0 | ',format(result['tn'],'5d'),' | ',format(result['fp'],'5d'))\n",
        "    print('gl -----------------------')\n",
        "    print('   1 | ',format(result['fn'],'5d'),' | ',format(result['tp'],'5d'))\n",
        "    print('---------------------------------------------------')\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    assert len(preds) == len(labels)\n",
        "\n",
        "    #round everything to 2 decimals\n",
        "    accuracy = round(np.mean(preds == labels) * 100, 2)\n",
        "    mcc = round(matthews_corrcoef(labels, preds) * 100, 2)\n",
        "\n",
        "    #Macro and weighted precision, recall and F1 scores\n",
        "    macro_p = round(precision_score(labels, preds, average='macro') * 100, 2)\n",
        "    macro_r = round(recall_score(labels, preds, average='macro') * 100, 2)\n",
        "    macro_f1 = round(f1_score(labels, preds, average='macro') * 100, 2)\n",
        "    w_macro_p = round(precision_score(labels, preds, average='weighted') * 100, 2)\n",
        "    w_macro_r = round(recall_score(labels, preds, average='weighted') * 100, 2)\n",
        "    w_macro_f1 = round(f1_score(labels, preds, average='weighted') * 100, 2)\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()\n",
        "    return (\n",
        "        {\n",
        "            **{\"accuracy\": accuracy,\n",
        "               \"Macro-P\": macro_p,\n",
        "              \"Macro-R\": macro_r,\n",
        "              \"Macro-F1\": macro_f1,\n",
        "              \"W Macro-P\": w_macro_p,\n",
        "              \"W Macro-R\": w_macro_r,\n",
        "              \"W Macro-F1\": w_macro_f1,\n",
        "              \"MCC\": mcc,\n",
        "              \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn},\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop for fine-tuning. Training was stopped once results started stagnating on the dev set (in this case, at the 4th epoch). You can specify where you want to save the model by changing the *location variable*."
      ],
      "metadata": {
        "id": "o4p43o6ITmpb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "dZAWX2UuTSPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4656c2f0-e407-445a-c7c2-0ab8de9ac024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs  10\n",
            "epoch  0\n",
            "batches:  1\n",
            "batch nr  0\n",
            "1\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 Training avg loss 0.697169303894043\n",
            "epoch 0 Dev set avg loss 0.7247381210327148\n",
            "{'accuracy': 38.46, 'Macro-P': 19.23, 'Macro-R': 50.0, 'Macro-F1': 27.78, 'W Macro-P': 14.79, 'W Macro-R': 38.46, 'W Macro-F1': 21.37, 'MCC': 0.0, 'tp': 5, 'tn': 0, 'fp': 8, 'fn': 0}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      0  |      8\n",
            "gl -----------------------\n",
            "   1 |      0  |      5\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "\n",
            "epoch  1\n",
            "batches:  1\n",
            "batch nr  0\n",
            "1\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 Training avg loss 0.6776463389396667\n",
            "epoch 1 Dev set avg loss 0.7238956689834595\n",
            "{'accuracy': 38.46, 'Macro-P': 19.23, 'Macro-R': 50.0, 'Macro-F1': 27.78, 'W Macro-P': 14.79, 'W Macro-R': 38.46, 'W Macro-F1': 21.37, 'MCC': 0.0, 'tp': 5, 'tn': 0, 'fp': 8, 'fn': 0}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      0  |      8\n",
            "gl -----------------------\n",
            "   1 |      0  |      5\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "\n",
            "epoch  2\n",
            "batches:  1\n",
            "batch nr  0\n",
            "1\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 Training avg loss 0.6801583170890808\n",
            "epoch 2 Dev set avg loss 0.7239802479743958\n",
            "{'accuracy': 38.46, 'Macro-P': 19.23, 'Macro-R': 50.0, 'Macro-F1': 27.78, 'W Macro-P': 14.79, 'W Macro-R': 38.46, 'W Macro-F1': 21.37, 'MCC': 0.0, 'tp': 5, 'tn': 0, 'fp': 8, 'fn': 0}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      0  |      8\n",
            "gl -----------------------\n",
            "   1 |      0  |      5\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "\n",
            "epoch  3\n",
            "batches:  1\n",
            "batch nr  0\n",
            "1\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 Training avg loss 0.6631960272789001\n",
            "epoch 3 Dev set avg loss 0.7245372533798218\n",
            "{'accuracy': 38.46, 'Macro-P': 19.23, 'Macro-R': 50.0, 'Macro-F1': 27.78, 'W Macro-P': 14.79, 'W Macro-R': 38.46, 'W Macro-F1': 21.37, 'MCC': 0.0, 'tp': 5, 'tn': 0, 'fp': 8, 'fn': 0}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      0  |      8\n",
            "gl -----------------------\n",
            "   1 |      0  |      5\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "\n",
            "epoch  4\n",
            "batches:  1\n",
            "batch nr  0\n",
            "1\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 Training avg loss 0.6675573587417603\n",
            "epoch 4 Dev set avg loss 0.7255657911300659\n",
            "{'accuracy': 38.46, 'Macro-P': 19.23, 'Macro-R': 50.0, 'Macro-F1': 27.78, 'W Macro-P': 14.79, 'W Macro-R': 38.46, 'W Macro-F1': 21.37, 'MCC': 0.0, 'tp': 5, 'tn': 0, 'fp': 8, 'fn': 0}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      0  |      8\n",
            "gl -----------------------\n",
            "   1 |      0  |      5\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "\n",
            "epoch  5\n",
            "batches:  1\n",
            "batch nr  0\n",
            "1\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 Training avg loss 0.6578407883644104\n",
            "epoch 5 Dev set avg loss 0.7267816066741943\n",
            "{'accuracy': 38.46, 'Macro-P': 19.23, 'Macro-R': 50.0, 'Macro-F1': 27.78, 'W Macro-P': 14.79, 'W Macro-R': 38.46, 'W Macro-F1': 21.37, 'MCC': 0.0, 'tp': 5, 'tn': 0, 'fp': 8, 'fn': 0}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      0  |      8\n",
            "gl -----------------------\n",
            "   1 |      0  |      5\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "\n",
            "epoch  6\n",
            "batches:  1\n",
            "batch nr  0\n",
            "1\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6 Training avg loss 0.6854851841926575\n",
            "epoch 6 Dev set avg loss 0.7278547286987305\n",
            "{'accuracy': 38.46, 'Macro-P': 19.23, 'Macro-R': 50.0, 'Macro-F1': 27.78, 'W Macro-P': 14.79, 'W Macro-R': 38.46, 'W Macro-F1': 21.37, 'MCC': 0.0, 'tp': 5, 'tn': 0, 'fp': 8, 'fn': 0}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      0  |      8\n",
            "gl -----------------------\n",
            "   1 |      0  |      5\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "\n",
            "epoch  7\n",
            "batches:  1\n",
            "batch nr  0\n",
            "1\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7 Training avg loss 0.6683212518692017\n",
            "epoch 7 Dev set avg loss 0.7289440631866455\n",
            "{'accuracy': 38.46, 'Macro-P': 19.23, 'Macro-R': 50.0, 'Macro-F1': 27.78, 'W Macro-P': 14.79, 'W Macro-R': 38.46, 'W Macro-F1': 21.37, 'MCC': 0.0, 'tp': 5, 'tn': 0, 'fp': 8, 'fn': 0}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      0  |      8\n",
            "gl -----------------------\n",
            "   1 |      0  |      5\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "\n",
            "epoch  8\n",
            "batches:  1\n",
            "batch nr  0\n",
            "1\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8 Training avg loss 0.7108211517333984\n",
            "epoch 8 Dev set avg loss 0.7299705743789673\n",
            "{'accuracy': 38.46, 'Macro-P': 19.23, 'Macro-R': 50.0, 'Macro-F1': 27.78, 'W Macro-P': 14.79, 'W Macro-R': 38.46, 'W Macro-F1': 21.37, 'MCC': 0.0, 'tp': 5, 'tn': 0, 'fp': 8, 'fn': 0}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      0  |      8\n",
            "gl -----------------------\n",
            "   1 |      0  |      5\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "\n",
            "epoch  9\n",
            "batches:  1\n",
            "batch nr  0\n",
            "1\n",
            "0\n",
            "epoch 9 Training avg loss 0.647945761680603\n",
            "epoch 9 Dev set avg loss 0.7304792404174805\n",
            "{'accuracy': 38.46, 'Macro-P': 19.23, 'Macro-R': 50.0, 'Macro-F1': 27.78, 'W Macro-P': 14.79, 'W Macro-R': 38.46, 'W Macro-F1': 21.37, 'MCC': 0.0, 'tp': 5, 'tn': 0, 'fp': 8, 'fn': 0}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      0  |      8\n",
            "gl -----------------------\n",
            "   1 |      0  |      5\n",
            "---------------------------------------------------\n",
            "---------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "print(\"epochs \", num_train_epochs)\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "    print(\"epoch \", epoch)\n",
        "    print(\"batches: \", len(train_dataloader))\n",
        "\n",
        "    for i,batch in enumerate(train_dataloader):\n",
        "        print(\"batch nr \", i)\n",
        "        batch = get_inputs_dict(batch)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "    #save the model\n",
        "    location = \"test-model.pt\"\n",
        "    # location = \"/content/drive/MyDrive/NLU/test-model\" + str(epoch) + \".pt\"\n",
        "    torch.save(model, location)\n",
        "\n",
        "    #evaluate model with dev_df at the end of the epoch.\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    n_batches = len(dev_dataloader)\n",
        "    preds = np.empty((len(dev_dataset), num_labels))\n",
        "    out_label_ids = np.empty((len(dev_dataset)))\n",
        "    model.eval()\n",
        "\n",
        "    print(len(dev_dataloader))\n",
        "    for i,dev_batch in enumerate(dev_dataloader):\n",
        "        with torch.no_grad():\n",
        "            print(i)\n",
        "            dev_batch = get_inputs_dict(dev_batch)\n",
        "            input_ids = dev_batch['input_ids'].to(device)\n",
        "            attention_mask = dev_batch['attention_mask'].to(device)\n",
        "            labels = dev_batch['labels'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "            eval_loss += tmp_eval_loss.item()\n",
        "\n",
        "        nb_eval_steps += 1\n",
        "        start_index = dev_batch_size * i\n",
        "        end_index = start_index + dev_batch_size if i != (n_batches - 1) else len(dev_dataset)\n",
        "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
        "        out_label_ids[start_index:end_index] = dev_batch[\"labels\"].detach().cpu().numpy()\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    model_outputs = preds\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    result = compute_metrics(preds, out_label_ids)\n",
        "\n",
        "    print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
        "    print('epoch',epoch,'Dev set avg loss',eval_loss)\n",
        "    print(result)\n",
        "    print_confusion_matrix(result)\n",
        "    print('---------------------------------------------------\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "VKX7oj-xJQFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load evaluation data with labels."
      ],
      "metadata": {
        "id": "WaVwx1WgJnlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read and process the data\n",
        "dev_location = 'dev.csv'\n",
        "print('\\n\\n\\nEvaluation data:')\n",
        "test_df = get_data(dev_location)\n",
        "\n",
        "#the batch size that gets processed at once during testing\n",
        "test_batch_size = 64\n",
        "\n",
        "#process the data in the required format\n",
        "test_examples = (test_df.iloc[:, 0].astype(str).tolist(), test_df.iloc[:, 1].tolist())\n",
        "test_dataset = NliDataset(test_examples,tokenizer)\n",
        "\n",
        "#split into batches\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SmaUA6wJZFO",
        "outputId": "8163bd1b-049f-47aa-ccde-3ae95598a37d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Evaluation data:\n",
            "\n",
            "Before:\n",
            "\n",
            "Columns:  Index(['premise', 'hypothesis', 'label'], dtype='object')\n",
            "\n",
            "First entry:\n",
            "  premise       He really shook up my whole mindset, Broker says. \n",
            "hypothesis               His mindset never changed, Broker said.\n",
            "label                                                          0\n",
            "Name: 1, dtype: object\n",
            "\n",
            "After:\n",
            "\n",
            "Columns:  Index(['text', 'label'], dtype='object')\n",
            "\n",
            "First entry:\n",
            "  text      <s> He really shook up my whole mindset, Broker says.  </s> His mindset never changed, Broker said. </s> \n",
            "label                                                                                                             0\n",
            "Name: 1, dtype: object\n",
            "torch.Size([13, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a model from Drive or from the file storage. (We can use the model from the previous step - just comment out this code in that case)."
      ],
      "metadata": {
        "id": "K8g3--g3Ju6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"/content/drive/MyDrive/NLU/roberta-model4.pt\"\n",
        "model = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
        "# model = torch.load(PATH)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoeBC45FKBpp",
        "outputId": "fdbbc2f5-7711-4d4c-eb19-1b7047e2cf44"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaClassifier(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make the predictions"
      ],
      "metadata": {
        "id": "qIORV3kJKKz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "eval_loss = 0.0\n",
        "nb_eval_steps = 0\n",
        "n_batches = len(test_dataloader)\n",
        "preds = np.empty((len(test_dataset), num_labels))\n",
        "out_label_ids = np.empty((len(test_dataset)))\n",
        "model.eval()\n",
        "\n",
        "print(len(test_dataloader))\n",
        "for i,test_batch in enumerate(test_dataloader):\n",
        "  with torch.no_grad():\n",
        "      if i%10 ==0:\n",
        "        print(i)\n",
        "      test_batch = get_inputs_dict(test_batch)\n",
        "      input_ids = test_batch['input_ids'].to(device)\n",
        "      attention_mask = test_batch['attention_mask'].to(device)\n",
        "      labels = test_batch['labels'].to(device)\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      tmp_eval_loss, logits = outputs[:2]\n",
        "      eval_loss += tmp_eval_loss.item()\n",
        "\n",
        "  nb_eval_steps += 1\n",
        "  start_index = test_batch_size * i\n",
        "  end_index = start_index + test_batch_size if i != (n_batches - 1) else len(test_dataset)\n",
        "  preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
        "  out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZMnRHyxKIRc",
        "outputId": "0c4b5710-483d-4395-8c04-5298648bbe93"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model"
      ],
      "metadata": {
        "id": "lVwfqhqVKQkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss = eval_loss / nb_eval_steps\n",
        "\n",
        "preds = np.argmax(preds, axis=1)\n",
        "print(preds)\n",
        "print(out_label_ids)\n",
        "\n",
        "result = compute_metrics(preds, out_label_ids)\n",
        "\n",
        "print(result)\n",
        "print_confusion_matrix(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1m-mFs5KSAI",
        "outputId": "4da38714-8854-4d5e-e9ae-a245134bcfe0"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 0 1 1 0 0 0 1 0 1 0]\n",
            "[0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "{'accuracy': 76.92, 'Macro-P': 76.19, 'Macro-R': 77.5, 'Macro-F1': 76.36, 'W Macro-P': 78.39, 'W Macro-R': 76.92, 'W Macro-F1': 77.2, 'MCC': 53.67, 'tp': 4, 'tn': 6, 'fp': 2, 'fn': 1}\n",
            "confusion matrix:\n",
            "            predicted    \n",
            "          0     |     1\n",
            "    ----------------------\n",
            "   0 |      6  |      2\n",
            "gl -----------------------\n",
            "   1 |      1  |      4\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1x2170GWvUvq6PDLdu_CFs5CeyL1Wwkuj",
      "authorship_tag": "ABX9TyNi80LglyVl2YZUyyFpert8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}